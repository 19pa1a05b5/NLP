{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXPERIMENT 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMRgy0ZHVhanPIrRZQU94o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19pa1a05b5/NLP/blob/main/EXPERIMENT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QnffGmC9UUI",
        "outputId": "78dccb6d-0a4d-418f-9137-732c0bb7d21e"
      },
      "source": [
        "#importing required libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs-9ooQd-Bm8"
      },
      "source": [
        "#sample text\n",
        "NLP='''Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'''"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEjZ3AmIAhP1"
      },
      "source": [
        "**1. Tokenize the given text into words and sentences by using three methods (split method, regular expressions and NLTK)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a39RWKKO_UPJ",
        "outputId": "3872650b-ec35-4ab3-c0cb-923cda4b3f53"
      },
      "source": [
        "#Word Tokenizations using three different methods.\n",
        "#split() method\n",
        "tokens1 = NLP.split()\n",
        "print(tokens1)\n",
        "print('No.of tokens1: ',len(tokens1))\n",
        "\n",
        "#Using RegEx \n",
        "tokens2 = re.findall(\"[\\w]+\", NLP)\n",
        "print(tokens2)\n",
        "print('No.of tokens2: ',len(tokens2))\n",
        "\n",
        "#Using NLTK\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens3 = word_tokenize(NLP)\n",
        "print(tokens3)\n",
        "print('No.of tokens3: ',len(tokens3))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'subfield', 'of', 'linguistics,', 'computer', 'science,', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language,', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '\"understanding\"', 'the', 'contents', 'of', 'documents,', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves.']\n",
            "No.of tokens1:  82\n",
            "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves']\n",
            "No.of tokens2:  82\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '``', 'understanding', \"''\", 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']\n",
            "No.of tokens3:  93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdE5BEixDZYg",
        "outputId": "ea9fa5d8-0d63-4c3b-c173-e97e27b23b65"
      },
      "source": [
        "#Sentence Tokenizations using three different methods.\n",
        "#split() method\n",
        "tokens_1 = NLP.split('.')\n",
        "print(tokens_1)\n",
        "print('No.of tokens_1: ',len(tokens_1))\n",
        "\n",
        "#Using RegEx \n",
        "tokens_2 = re.compile('[.?!] ').split(NLP)\n",
        "print(tokens_2)\n",
        "print('No.of tokens_2: ',len(tokens_2))\n",
        "\n",
        "#Using NLTK\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokens_3 = sent_tokenize(NLP)\n",
        "print(tokens_3)\n",
        "print('No.of tokens_3: ',len(tokens_3))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data', ' The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them', ' The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves', '']\n",
            "No.of tokens_1:  4\n",
            "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data', 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them', 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.']\n",
            "No.of tokens_2:  3\n",
            "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.', 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.', 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.']\n",
            "No.of tokens_3:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJJAhSrRFAJl"
      },
      "source": [
        "**2. Perform stemming on the tokens present in the given sentence**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0DeIshHFI7f",
        "outputId": "387fa4c2-e208-4b1e-ff9b-ccb9e1d19e6e"
      },
      "source": [
        "#Stemming using PorterStemmer method\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "sentence = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "def stem(S):\n",
        "  tokens=word_tokenize(S)\n",
        "  res=\"\"\n",
        "  for i in tokens:\n",
        "    res+=porter.stem(i)+\" \"\n",
        "  return res\n",
        "print(\"Result after stemming: \", stem(sentence))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after stemming:  python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN7oN0-cHWRr"
      },
      "source": [
        "**3. Perform lemmatization on the tokens present in the give sentence.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw2l0blxHb_V",
        "outputId": "506a0e1a-2e8b-4860-92a0-1fd4edbb5fae"
      },
      "source": [
        "#Lemmatization using WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "def lemma(S):\n",
        "  token_words = nltk.word_tokenize(S)\n",
        "  res=\"\"\n",
        "  for i in token_words:\n",
        "    res+=wordnet.lemmatize(i)+\" \"\n",
        "  return res\n",
        "print(\"Result after lemmatization: \", lemma(sentence))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result after lemmatization:  He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
          ]
        }
      ]
    }
  ]
}